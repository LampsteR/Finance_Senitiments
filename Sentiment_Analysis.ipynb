{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOT2I9byH/2Wq2PnRlVk5Qt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LampsteR/Finance_Senitiments/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AuTJ1JQrl9D"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "miq1ykjfr7xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = '/content/drive/MyDrive/merged_financial_sentiment.json'  # update path\n",
        "\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    for _ in range(5):          # print first 5 entries\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        record = json.loads(line)\n",
        "        print(record)"
      ],
      "metadata": {
        "id": "PJkqBZtcsqDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers\n",
        "\n",
        "# import json\n",
        "# from pathlib import Path\n",
        "\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# # -------------------------------------------------------------------\n",
        "# # 1. Read the newline-delimited JSON file (one record per line)\n",
        "# # -------------------------------------------------------------------\n",
        "# def load_jsonl(path):\n",
        "#     path = Path(path)\n",
        "#     records = []\n",
        "#     with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             if line.strip():\n",
        "#                 records.append(json.loads(line))\n",
        "#     return records\n",
        "\n",
        "# records = load_jsonl(\"/content/drive/MyDrive/merged_financial_sentiment.json\")\n",
        "\n",
        "# # -------------------------------------------------------------------\n",
        "# # 2. Normalize labels (string → int, sorted order: negative < neutral < positive)\n",
        "# # -------------------------------------------------------------------\n",
        "# label_order = [\"negative\", \"neutral\", \"positive\"]\n",
        "# label_to_id = {label: idx for idx, label in enumerate(label_order)}\n",
        "\n",
        "# def normalize_label(raw_label):\n",
        "#     if isinstance(raw_label, str):\n",
        "#         return label_to_id[raw_label.lower()]\n",
        "#     return int(raw_label)\n",
        "\n",
        "# texts = [record[\"text\"] for record in records]\n",
        "# labels = [normalize_label(record[\"label\"]) for record in records]\n",
        "\n",
        "# # -------------------------------------------------------------------\n",
        "# # 3. Tokenize with FinBERT (padding/truncation to 256 tokens)\n",
        "# # -------------------------------------------------------------------\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# encodings = tokenizer(\n",
        "#     texts,\n",
        "#     padding=\"max_length\",\n",
        "#     truncation=True,\n",
        "#     max_length=256,\n",
        "#     return_tensors=\"pt\"\n",
        "# )\n",
        "\n",
        "# # -------------------------------------------------------------------\n",
        "# # 4. Keep only the tensors you need\n",
        "# # -------------------------------------------------------------------\n",
        "# dataset = {\n",
        "#     \"input_ids\": encodings[\"input_ids\"],\n",
        "#     \"attention_mask\": encodings[\"attention_mask\"],\n",
        "#     \"labels\": torch.tensor(labels)\n",
        "# }\n",
        "\n",
        "# print(dataset[\"input_ids\"].shape, dataset[\"attention_mask\"].shape, dataset[\"labels\"].shape)"
      ],
      "metadata": {
        "id": "ggf1D_FpuYe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  #  output_path = '/content/drive/MyDrive/Sentiment_tokens/finbert_inputs.pt'  # adjust folder/name\n",
        "  #  torch.save(dataset, output_path)\n",
        "  #  print(f'Saved tensors to {output_path}')"
      ],
      "metadata": {
        "id": "7ARk3LsD1MW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install -U bitsandbytes --extra-index-url https://download.pytorch.org/whl/cu124\n",
        "  !pip show bitsandbytes"
      ],
      "metadata": {
        "id": "hXMDmLgK3NHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 0. Mount Drive once per session\n",
        "# --------------------------------------------------------------\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1. Load the tokenized tensors you saved earlier\n",
        "# --------------------------------------------------------------\n",
        "dataset = torch.load('/content/drive/MyDrive/Sentiment_tokens/finbert_inputs.pt')\n",
        "input_ids = dataset['input_ids']\n",
        "attention_mask = dataset['attention_mask']\n",
        "labels = dataset['labels']\n",
        "num_labels = labels.unique().numel()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2. Set up 4-bit (QLoRA) base model\n",
        "# --------------------------------------------------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'ProsusAI/finbert',\n",
        "    num_labels=num_labels,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3. Configure and attach LoRA adapters\n",
        "# --------------------------------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias='none',\n",
        "    target_modules=['query', 'value'],\n",
        "    task_type='SEQ_CLS',\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# `model` is now ready for training with your tokenized tensors."
      ],
      "metadata": {
        "id": "6cyRkTUY19qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you’re in a fresh runtime/cell, load the saved tensors again\n",
        "dataset = torch.load('/content/drive/MyDrive/Sentiment_tokens/finbert_inputs.pt')\n",
        "input_ids = dataset['input_ids']\n",
        "attention_mask = dataset['attention_mask']\n",
        "labels = dataset['labels']\n",
        "\n",
        "class FinSentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx],\n",
        "        }\n",
        "\n",
        "train_dataset = FinSentimentDataset(input_ids, attention_mask, labels)"
      ],
      "metadata": {
        "id": "jzvWyOrg_kyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Build the training dataset (uses the tensors loaded earlier)\n",
        "train_dataset = FinSentimentDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "# Detect bf16 support (Ampere+ GPUs); fall back to fp16 otherwise\n",
        "bf16_available = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/finbert-qlora-checkpoints',\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to='none',\n",
        "    fp16=False ,\n",
        "    bf16=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eB_ZIdO83DbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load saved tensors ----------------------------------------------------------\n",
        "bundle = torch.load('/content/drive/MyDrive/Sentiment_tokens/finbert_inputs.pt')\n",
        "input_ids = bundle['input_ids']\n",
        "attention_mask = bundle['attention_mask']\n",
        "labels = bundle['labels']\n",
        "num_labels = labels.unique().numel()\n",
        "\n",
        "# Build dict-returning dataset (avoids vars() error) --------------------------\n",
        "class FinSentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx],\n",
        "        }\n",
        "\n",
        "train_dataset = FinSentimentDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "# Set up QLoRA model ----------------------------------------------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'ProsusAI/finbert',\n",
        "    num_labels=num_labels,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias='none',\n",
        "    target_modules=['query', 'value'],\n",
        "    task_type='SEQ_CLS',\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "import os\n",
        "from accelerate.state import AcceleratorState\n",
        "\n",
        "AcceleratorState._reset_state()\n",
        "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/finbert-qlora-checkpoints',\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to='none',\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Training hyperparameters (no mixed precision) -------------------------------\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/finbert-qlora-checkpoints',\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to='none',\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "OIzWodWU7uXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import PeftModel\n",
        "\n",
        "# 1. Point to your saved adapter directory\n",
        "\n",
        "\n",
        "\n",
        "adapter_dir = \"/content/drive/MyDrive/finbert-qlora-checkpoints/checkpoint-4311\"  # adjust if the folder is elsewhere\n",
        "\n",
        "print(\"adapter_dir =\", adapter_dir, type(adapter_dir))\n",
        "\n",
        "import os\n",
        "print(\"exists?\", os.path.isdir(adapter_dir))\n",
        "\n",
        "print(\"adapter_dir =\", adapter_dir, type(adapter_dir))\n",
        "# 2. Load tokenizer and label order\n",
        "base_model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "label_names = [\"negative\", \"neutral\", \"positive\"]  # ensure this matches the order used during training\n",
        "\n",
        "# 3. Load base FinBERT + LoRA adapter\n",
        "base_model_name = \"ProsusAI/finbert\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=len(label_names),\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapter_dir)\n",
        "model.eval()\n",
        "\n",
        "# 4. Helper to score text\n",
        "def classify_sentiment(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        pred_ids = probs.argmax(dim=-1).cpu().tolist()\n",
        "        predictions = [label_names[i] for i in pred_ids]\n",
        "        confidences = probs.max(dim=-1).values.cpu().tolist()\n",
        "    return list(zip(texts, predictions, confidences))\n",
        "\n",
        "# 5. Try a few queries\n",
        "queries = [\n",
        "    \"The company posted record profits and raised guidance.\",\n",
        "    \"Regulators opened an investigation into the firm's accounting practices.\",\n",
        "    \"Management expects performance to remain stable next quarter.\"\n",
        "]\n",
        "\n",
        "for text, label, confidence in classify_sentiment(queries):\n",
        "    print(f\"Text: {text}\\nPrediction: {label} (confidence={confidence:.3f})\\n\")"
      ],
      "metadata": {
        "id": "OzHDkuAwpvod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDpPfShoqAXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}